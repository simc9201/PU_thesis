# -*- coding: utf-8 -*-
"""datapreprocessing_clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eitdeKN1gxuY84g0uRMVu8iFnNW680gv

# Load needed packages
"""

pip install geopandas

import os 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import geopandas as gpd 
import geopy
from scipy import stats 

from PIL import Image
from wordcloud import WordCloud

"""# Load dataset from Google Drive"""

from google.colab import drive
drive.mount('/content/gdrive')

os.chdir('/content/gdrive/MyDrive/')

data = pd.read_csv("subsetdata.csv")

data

"""# Dataset overview"""

len(data.columns) #54 features, 10000 rows

data.info() #find a way to display information about variables, types, and null values 
#data.iloc[:, 0:12].describe() 
#data.isnull().sum() 
#data.corr()

"""# Converting continuous target variable to binary """

data_rm = data.dropna(subset=['song_hotttnesss']) #remove null values from target variable 
data_rm = data_rm.reset_index(drop=True) #reset index in dataset

data_rm.info()
data_rm = data_rm.dropna(subset=['artist_familiarity']) #remove single missing value in artist_familiarity

data_rm.iloc[:, 13].describe() #distribution and quantiles of song_hotttnesss

# set threshold of binary target variable to 75th percentile
threshold = data_rm.song_hotttnesss.quantile(0.75) 

#convert to bool labels hit and nonhit
data_rm['hit_song'] = data_rm['song_hotttnesss'] > threshold
#data_rmnull['hit_song'] = (data_rmnull['song_hotttnesss'] > threshold).astype(int) #to convert to 0 and 1
data_rm['hit_song'] = data_rm['hit_song'].map({True:'Hit' ,False:'Non-hit'})

#sanity check 
data_rm['hit_song'].value_counts() #75% non-hit, 25% hit

#pie chart of target class distribution
labels = ['Non-Hit Songs', 'Hit Songs'] 
counts = data_rmnull['hit_song'].value_counts() 
plt.pie(counts, labels=labels, colors = ['skyblue', 'lightgreen'], autopct = lambda x: f'{x:.0f}%') 
plt.title('Songs') 
plt.show()

"""# Explore array type features and transform into mean and variance variables"""

import statistics

#function returns average and variance of a list, where each list = 1 observation
def avg_and_variance_funct(list):
  list_clean = list.replace("\n", "").replace("[", "").replace("]", "").split(" ")
  int_list = []
  if len(list_clean) > 1: 
    for item in list_clean:
      if (item != '' and item != '[' and item != '...'):
        int_list.append(float(item))
  
    avg = statistics.mean(int_list)
    var = statistics.variance(int_list)
  
  if len(list_clean) == 1:
    avg = 0
    var = 0
  
  return avg, var


#function returns list of average and variance for all data in a single column
def iter_every_row(data_column):
  mean_list = []
  variance_list = []

  for list in data_column: 
    avg, var = avg_and_variance_funct(list)
    mean_list.append(avg)
    variance_list.append(var)
  
  return mean_list, variance_list

#find average and variance of each feature
avg_seg_start, var_seg_start = iter_every_row(data_rm["segments_start"])
avg_seg_conf, var_seg_conf = iter_every_row(data_rm["segments_confidence"])

avg_sec_start, var_sec_start = iter_every_row(data_rm["sections_start"])
avg_sec_conf, var_sec_conf = iter_every_row(data_rm["sections_confidence"])

avg_beats_start, var_beats_start = iter_every_row(data_rm["beats_start"])
avg_beats_conf, var_beats_conf = iter_every_row(data_rm["beats_confidence"])

avg_bars_start, var_bars_start = iter_every_row(data_rm["bars_start"])
avg_bars_conf, var_bars_conf = iter_every_row(data_rm["bars_confidence"])

avg_tatums_start, var_tatums_start = iter_every_row(data_rm["tatums_start"])
avg_tatums_conf, var_tatums_conf = iter_every_row(data_rm["tatums_confidence"])

avg_seg_loud_max, var_seg_loud_max = iter_every_row(data_rm["segments_loudness_max"])
avg_seg_loud_start, var_seg_loud_start = iter_every_row(data_rm["segments_loudness_start"])

#add each avg and variance feature to dataset for each _start and _confidence feature 
data_rm["segments_start_mean"] = avg_seg_start
data_rm["segments_start_variance"] = var_seg_start
data_rm["segments_confidence_mean"] = avg_seg_conf
data_rm["segments_confidence_variance"] = var_seg_conf

data_rm["sections_start_mean"] = avg_sec_start
data_rm["sections_start_variance"] = var_sec_start
data_rm["sections_confidence_mean"] = avg_sec_conf
data_rm["sections_confidence_variance"] = var_sec_conf

data_rm["beats_start_mean"] = avg_beats_start
data_rm["beats_start_variance"] = var_beats_start
data_rm["beats_confidence_mean"] = avg_beats_conf
data_rm["beats_confidence_variance"] = var_beats_conf

data_rm["bars_start_mean"] = avg_bars_start
data_rm["bars_start_variance"] = var_bars_start
data_rm["bars_confidence_mean"] = avg_bars_conf
data_rm["bars_confidence_variance"] = var_bars_conf

data_rm["tatums_start_mean"] = avg_tatums_start
data_rm["tatums_start_variance"] = var_tatums_start
data_rm["tatums_confidence_mean"] = avg_tatums_conf
data_rm["tatums_confidence_variance"] = var_tatums_conf

data_rm["segments_loudness_max_mean"] = avg_seg_loud_max
data_rm["segments_loudness_max_variance"] = var_seg_loud_max
data_rm["segments_loudness_start_mean"] = avg_seg_loud_start
data_rm["segments_loudness_start_variance"] = var_seg_loud_start

#remove original features from dataset
data_rm = data_rm.drop(columns=['segments_start', 'segments_confidence', 
                                'sections_start', 'sections_confidence', 
                                'beats_start', 'beats_confidence', 
                                'bars_start', 'bars_confidence', 
                                'tatums_start', 'tatums_confidence', 
                                'segments_loudness_max', 
                                'segments_loudness_max_time', 
                                'segments_loudness_start']) #remove features after creating mean and var

"""# Removing irrelevant variables and converting mode to binary"""

#removing irrelevant variables
data_rm = data_rm.drop(columns=['artist_location', 'artist_name', 'release', 
                                    'title', 'artist_id', 'artist_mbid', 
                                    'artist_playmeid', 'artist_7digitalid', 
                                    'song_id', 'release_7digitalid', 
                                    'track_7digitalid', 'track_id', 'audio_md5', 
                                    'analysis_sample_rate', 'segments_pitches', 
                                    'segments_timbre'])

#converting mode to binary variable 
data_rm["mode"].value_counts()
data_rm["mode"] = data_rm['mode'].map({0:'Minor' , 1:'Major'})

"""# Exporting dataset to google drive """

#takes about 6 mins to run 
drive.mount('/content/gdrive')
data_rm.to_csv('/content/gdrive/MyDrive/edittedsubsetdata.csv')